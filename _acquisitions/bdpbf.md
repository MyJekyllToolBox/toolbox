---
title: "Big Data Pipelines Deployment Framework (BDPDF)"
date: 2018-11-18T12:33:46+10:00
weight: 5
description: "A framework to allow high-level design/specification of scalability aspects of Big Data processing pipelines and their deployment on the continuum computing infrastructure."
license: "https://img.shields.io/badge/license-Apache2.0-blue"
trl: "https://img.shields.io/badge/TRL-8-green"
github: "https://github.com/SINTEF-9012/ebw-prototype"
---

A framework to allow high-level design/specification of scalability aspects of Big Data processing pipelines and their deployment on the continuum computing infrastructure.

## Name
Big Data Pipelines Deployment Framework
## Defined in Task
n/a
## Short description
A framework to allow high-level design/specification of scalability aspects of Big Data processing pipelines and their effective and efficient deployment and execution on the continuum computing infrastructure (heterogenous Cloud/Fog/Edge infrastructure).

The purpose of such a framework is to lower the technological barriers of entry to the incorporation of Big Data pipelines in organizations' business processes regardless of the hardware infrastructure. The framework requires new languages, methods, infrastructures, and software for managing Big Data pipelines such that Big Data pipelines can be easily set up in a manner which is trace-able, manageable, analyzable and optimizable and separates the design- from the run-time aspects of their deployment, thus empowering domain experts to take an active part in their definition.

The purpose of such a framework is to allow specification of data processing pipelines by non-IT experts at an abstraction level suitable for pure data processing, in which pipeline specifications are realized (deployed and executed) using instances of a pre-defined set of scalable and composable software container templates (corresponding to step types in pipelines).